{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Check if data is already there and only download if it isnt\n",
    "if not os.path.exists('optdigits.tra') and not os.path.exists('optdigits.tes'):\n",
    "\n",
    "    # URLs for the dataset\n",
    "    train_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\"\n",
    "    test_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes\"\n",
    "\n",
    "    # Download and save the training data\n",
    "    urllib.request.urlretrieve(train_url, 'optdigits.tra')\n",
    "\n",
    "    # Download and save the test data\n",
    "    urllib.request.urlretrieve(test_url, 'optdigits.tes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    inputs = (data[:, :-1] - 8) / 8.0 # Normalize the inputs\n",
    "    targets = np.eye(10)[data[:, -1].astype(int)]  # One-hot encoding\n",
    "    return list(zip(inputs, targets))\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = load_data('optdigits.tra')\n",
    "test_data = load_data('optdigits.tes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.   , -0.875, -0.25 ,  0.875,  0.5  , -0.875, -1.   , -1.   ,\n",
       "        -1.   , -0.125,  1.   , -0.25 , -0.25 ,  0.25 , -1.   , -1.   ,\n",
       "        -1.   ,  0.   ,  1.   , -0.75 , -1.   ,  0.375, -0.75 , -1.   ,\n",
       "        -1.   , -0.375,  1.   , -0.625, -1.   , -0.375, -0.125, -1.   ,\n",
       "        -1.   , -0.125,  0.625, -0.625, -1.   ,  0.   , -0.125, -1.   ,\n",
       "        -1.   , -0.5  ,  0.5  , -1.   , -0.875,  0.625, -0.375, -1.   ,\n",
       "        -1.   , -1.   ,  0.75 ,  0.125,  0.875,  0.125, -1.   , -1.   ,\n",
       "        -1.   , -1.   , -0.25 ,  0.75 , -0.125, -0.875, -1.   , -1.   ]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to draw images\n",
    "def draw_images(data, num_images):\n",
    "    for index, (image, label) in enumerate(data[:num_images]):\n",
    "        plt.subplot(1, num_images, index + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image.reshape((8,8)), cmap=plt.cm.gray_r)\n",
    "        label = np.argmax(label)\n",
    "        plt.title('Label: %i' % label, fontsize = 12)\n",
    "    plt.show()\n",
    "\n",
    "# Draw some images from the training data\n",
    "draw_images(train_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def data_generator(data, minibatch_size):\n",
    "    random.shuffle(data)\n",
    "    for i in range(0, len(data), minibatch_size):\n",
    "        for i in range(0, len(data), minibatch_size):\n",
    "            minibatch = data[i:i+minibatch_size]\n",
    "            inputs = np.array([item[0] for item in minibatch])\n",
    "            targets = np.array([item[1] for item in minibatch])\n",
    "            yield inputs, targets\n",
    "\n",
    "# Create a data generator with minibatch size of 10\n",
    "train_data_gen = data_generator(train_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2, 2.3 Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"Abstract module class that NN are composed off\"\"\"\n",
    "    def __call__(self, input):\n",
    "        # Forward pass/inference\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def back(self, grad):\n",
    "        # Backward pass/derivative\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        # Return list of parameters\n",
    "        return []\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    def __call__(self, input):\n",
    "        self.output = 1 / (1 + np.exp(-input))\n",
    "        return self.output\n",
    "    \n",
    "    def back(self, grad):\n",
    "        return grad * self.output * (1 - self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    def __call__(self, input):\n",
    "        exp = np.exp(input - np.max(input, axis=-1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def back(self, grad):\n",
    "        return grad * self.output * (1 - self.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"Parameter class for NN parameters\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)\n",
    "        #self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.grad = np.zeros_like(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"Linear layer\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = Parameter(np.random.randn(input_size, output_size) * 0.01)\n",
    "        self.bias = Parameter(np.zeros(output_size))\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.weights.data) + self.bias.data\n",
    "        return self.output\n",
    "    \n",
    "    def back(self, grad):\n",
    "        self.weights.grad = np.dot(self.input.T, grad)\n",
    "        self.bias.grad = np.sum(grad, axis=0)\n",
    "        return np.dot(grad, self.weights.data.T)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + '(in=' + str(self.weights.data.shape[0]) + ', out=' + str(self.weights.data.shape[1]) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    \"\"\"Simple model\"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = [Linear(64, 32), Sigmoid(), Linear(32, 10), Softmax()]\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "    def back(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.back(grad)\n",
    "        return grad\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.reset()\n",
    "\n",
    "    def __str__(self):\n",
    "        total_params = sum(np.prod(param.data.shape) for layer in self.layers for param in layer.parameters())\n",
    "        return \"Model(\\n  \" + \"\\n  \".join([str(layer) for layer in self.layers]) + \"\\n), Param count: \" + str(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 CCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CCE:\n",
    "    \"\"\"Categorical cross-entropy loss\"\"\"\n",
    "    \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        # Clip prediction to avoid log(0) error\n",
    "        y_pred = np.clip(y_pred, 1e-9, 1. - 1e-9)\n",
    "        # Compute categorical cross-entropy loss\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return -np.sum(y_true * np.log(y_pred))\n",
    "    \n",
    "    def back(self):\n",
    "        return self.y_pred - self.y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass\n",
    "model = Model()\n",
    "cce = CCE()\n",
    "x, y = next(train_data_gen)\n",
    "output = model(x)\n",
    "assert output.shape == (10, 10)\n",
    "\n",
    "# test backward pass\n",
    "grad = cce(output, y)\n",
    "grad = model.back(grad)\n",
    "assert grad.shape == (10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  Linear(in=64, out=32)\n",
      "  Sigmoid()\n",
      "  Linear(in=32, out=10)\n",
      "  Softmax()\n",
      "), Param count: 2410\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Abstract optimizer class\"\"\"\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic gradient descent optimizer\"\"\"\n",
    "    def __init__(self, parameters, lr=1e-2):\n",
    "        super().__init__(parameters)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.parameters():\n",
    "            #print(\"Data\", param.data)\n",
    "            #print(\"Grad\", param.grad)\n",
    "            param.data -= self.lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, loss, optimizer, x, y):\n",
    "    output = model(x)\n",
    "    loss(output, y)\n",
    "    grad = loss.back()\n",
    "    model.back(grad)\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "    #optimizer.step()\n",
    "\n",
    "def training_loop(model, loss, optimizer, train_data, test_data, epochs, minibatch_size):\n",
    "    for epoch in range(epochs):\n",
    "        train_data_gen = data_generator(train_data, minibatch_size)\n",
    "        test_data_gen = data_generator(test_data, minibatch_size)\n",
    "        \n",
    "        # Training\n",
    "        for x, y in train_data_gen:\n",
    "            training_step(model, loss, optimizer, x, y)\n",
    "            \n",
    "        # Testing\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for x, y in test_data_gen:\n",
    "            output = model(x)\n",
    "            correct += np.sum(np.argmax(output, axis=-1) == np.argmax(y, axis=-1))\n",
    "            total += len(x)\n",
    "        print(f'Epoch: {epoch+1}, Accuracy: {correct/total:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = Model()\n",
    "cce = CCE()\n",
    "optimizer = SGD(model.parameters, lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = data_generator(train_data, 1)\n",
    "for i, (x, y) in enumerate(train_data_gen):\n",
    "    training_step(model, cce, optimizer, x, y)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Accuracy: 0.840\n",
      "Epoch: 2, Accuracy: 0.921\n",
      "Epoch: 3, Accuracy: 0.923\n",
      "Epoch: 4, Accuracy: 0.907\n",
      "Epoch: 5, Accuracy: 0.916\n",
      "Epoch: 6, Accuracy: 0.925\n",
      "Epoch: 7, Accuracy: 0.925\n",
      "Epoch: 8, Accuracy: 0.932\n",
      "Epoch: 9, Accuracy: 0.934\n",
      "Epoch: 10, Accuracy: 0.938\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, cce, optimizer, train_data, test_data, epochs=10, minibatch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_subset = test_data[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data point 1:\n",
      "Prediction: 7\n",
      "Actual target: 7\n",
      "\n",
      "Data point 2:\n",
      "Prediction: 5\n",
      "Actual target: 5\n",
      "\n",
      "Data point 3:\n",
      "Prediction: 6\n",
      "Actual target: 6\n",
      "\n",
      "Data point 4:\n",
      "Prediction: 5\n",
      "Actual target: 5\n",
      "\n",
      "Data point 5:\n",
      "Prediction: 8\n",
      "Actual target: 8\n",
      "\n",
      "Data point 6:\n",
      "Prediction: 5\n",
      "Actual target: 5\n",
      "\n",
      "Data point 7:\n",
      "Prediction: 5\n",
      "Actual target: 5\n",
      "\n",
      "Data point 8:\n",
      "Prediction: 9\n",
      "Actual target: 9\n",
      "\n",
      "Data point 9:\n",
      "Prediction: 4\n",
      "Actual target: 4\n",
      "\n",
      "Data point 10:\n",
      "Prediction: 7\n",
      "Actual target: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pred = model(test_data_subset[i][0])\n",
    "    print(f\"Data point {i+1}:\")\n",
    "    print(f\"Prediction: {np.argmax(pred)}\")\n",
    "    print(f\"Actual target: {np.argmax(test_data_subset[i][1])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
